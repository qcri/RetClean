{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71fb3551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "BACKEND_URL = \"http://localhost:8000\"  # FastAPI backend (Docker exposes this)\n",
    "ES_URL = \"http://localhost:9200\"\n",
    "QDRANT_URL = \"http://localhost:6333\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd4ff30",
   "metadata": {},
   "source": [
    "#### When using local model (LLAMA 3.1) for example\n",
    "\n",
    "Run the following after docker compose build has fully run:\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama ollama pull llama3.1\n",
    "```\n",
    "\n",
    "Then run:\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama ollama list\n",
    "```\n",
    "\n",
    "You should see something like:  \n",
    "`llama3.1:latest    46e0c10c039e    4.9 GB    3 seconds ago`\n",
    "\n",
    "Finally, run:\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama ollama run llama3.1 \"Say hello from inside Docker.\"\n",
    "```\n",
    "\n",
    "If it returns valid text then you have enough GPU VRAM to run local model otherwise it will print out the error. \n",
    "Common Error:  \n",
    "`Error: 500 Internal Server Error: model requires more system memory than is currently available unable to load full model on GPU`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83571852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Waiting for Backend to start...\n",
      "✅ Backend is up!\n",
      "⏳ Waiting for Elasticsearch to start...\n",
      "✅ Elasticsearch is up!\n",
      "⏳ Waiting for Qdrant to start...\n",
      "✅ Qdrant is up!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wait_for_service(url: str, name: str, timeout=120):\n",
    "    print(f\"⏳ Waiting for {name} to start...\")\n",
    "    start = time.time()\n",
    "    while time.time() - start < timeout:\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            if r.status_code in (200, 400):\n",
    "                print(f\"✅ {name} is up!\")\n",
    "                return True\n",
    "        except Exception:\n",
    "            time.sleep(3)\n",
    "    raise TimeoutError(f\"{name} did not start within {timeout}s\")\n",
    "\n",
    "wait_for_service(f\"{BACKEND_URL}/docs\", \"Backend\")\n",
    "wait_for_service(f\"{ES_URL}\", \"Elasticsearch\")\n",
    "wait_for_service(f\"{QDRANT_URL}/collections\", \"Qdrant\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de4cb444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# STEP 1: Define dummy data tables to simulate real-world inputs\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# 'table_1' represents a small dataset containing fruit names and their colors.\n",
    "# Each row is identified by a unique 'id'. This could represent, for example,\n",
    "# data from a specific database table or source file.\n",
    "table_1 = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3],                              # Unique identifiers for each record\n",
    "    \"fruit\": [\"apple\", \"banana\", \"mango\"],        # Fruit names\n",
    "    \"color\": [\"red\", \"yellow\", \"orange\"]          # Known color labels for each fruit\n",
    "})\n",
    "\n",
    "# 'table_2' is a second dataset following the same structure as 'table_1'.\n",
    "# This simulates appending or combining multiple related tables into a single index.\n",
    "# Each entry again has an 'id', a fruit name, and its corresponding color.\n",
    "table_2 = pd.DataFrame({\n",
    "    \"id\": [1, 2, 4],                              # Unique IDs for this second table\n",
    "    \"fruit\": [\"grape\", \"pineapple\", \"kiwi\"],      # Additional fruit names\n",
    "    \"color\": [\"purple\", \"brown\", \"green\"]         # Their known colors\n",
    "})\n",
    "\n",
    "# 'test_repair' simulates an *incomplete* dataset that requires data repair or imputation.\n",
    "# In this table:\n",
    "#   - Some 'color' values are missing (None)\n",
    "#   - One row intentionally contains an incorrect color (\"blue\" for banana)\n",
    "# This dataset will later be passed to the repair endpoint to infer or fix the missing values.\n",
    "test_repair = pd.DataFrame({\n",
    "    \"fruit\": [\"apple\", \"banana\", \"avocado\", \"lime\"],  # Fruits to repair or validate\n",
    "    \"color\": [None, \"blue\", None, None]               # Missing or incorrect color values\n",
    "})\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 2: Save the tables as CSV files for backend upload\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# The backend expects CSV files to simulate uploaded tables. \n",
    "# These are written to disk so they can be sent as file uploads in subsequent API calls.\n",
    "table_1.to_csv(\"table_1.csv\", index=False)       # Save the first dataset\n",
    "table_2.to_csv(\"table_2.csv\", index=False)       # Save the second dataset\n",
    "test_repair.to_csv(\"test_repair.csv\", index=False)  # Save the dataset to repair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3641706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE: 200 {\"status\":\"success\"}\n",
      "UPDATE: 200 {\"status\":\"success\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 0: Backend connection setup\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# The base URL of your running FastAPI backend. \n",
    "# This backend exposes endpoints for indexing and repairing data.\n",
    "BACKEND_URL = \"http://localhost:8000\"\n",
    "\n",
    "# Define a unique name for your combined index.\n",
    "# This index will store semantic and syntactic representations of the uploaded tables\n",
    "# inside Elasticsearch and Qdrant (both managed through your backend).\n",
    "INDEX_NAME = \"fruit_index_combined_final\"\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 1: CREATE a new index and upload the first CSV\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# The `/index/` endpoint handles both creation of a new index and\n",
    "# initial population with one or more CSV files.\n",
    "# Here we start by uploading the first dataset (`table_1.csv`).\n",
    "create_url = f\"{BACKEND_URL}/index/\"\n",
    "\n",
    "# Open the first CSV file in binary mode ('rb') since files must be uploaded\n",
    "# as multipart form-data for FastAPI to process them correctly.\n",
    "with open(\"table_1.csv\", \"rb\") as f1:\n",
    "    # Send a POST request to the backend to create a new index\n",
    "    # and populate it with the first dataset.\n",
    "    # - `data` sends form fields (here the index name).\n",
    "    # - `files` sends the actual CSV file as multipart upload.\n",
    "    resp = requests.post(\n",
    "        create_url,\n",
    "        data={\"index_name\": INDEX_NAME},\n",
    "        files={\"files\": f1}\n",
    "    )\n",
    "\n",
    "# Print backend response to confirm whether index creation was successful.\n",
    "# Expected: HTTP 200 OK (or 400 if index already exists)\n",
    "print(\"CREATE:\", resp.status_code, resp.text)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 2: UPDATE (append) the same index with another CSV\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# The same `/index/` endpoint supports PUT requests to *append* new tables\n",
    "# to an existing index. This allows you to combine multiple datasets into\n",
    "# one searchable index.\n",
    "update_url = f\"{BACKEND_URL}/index/\"\n",
    "\n",
    "# Prepare additional CSVs you want to merge into the same index.\n",
    "# Each file will be opened and attached as multipart form-data.\n",
    "files_to_add = [\"table_2.csv\"]\n",
    "\n",
    "# The 'files' list stores tuples in the format (field_name, file_object),\n",
    "# which is required by `requests.put()` to handle multiple uploads.\n",
    "files = [(\"files\", open(fname, \"rb\")) for fname in files_to_add]\n",
    "\n",
    "try:\n",
    "    # Send a PUT request to append these new files to the existing index.\n",
    "    # The backend will:\n",
    "    #   1. Load and embed the CSV data using the SentenceTransformer model.\n",
    "    #   2. Store textual data in Elasticsearch for syntactic search.\n",
    "    #   3. Store vector embeddings in Qdrant for semantic search.\n",
    "    resp = requests.put(\n",
    "        update_url,\n",
    "        data={\"index_name\": INDEX_NAME},\n",
    "        files=files\n",
    "    )\n",
    "\n",
    "    # Print backend response to confirm successful update.\n",
    "    # Expected: HTTP 200 OK on success.\n",
    "    print(\"UPDATE:\", resp.status_code, resp.text)\n",
    "\n",
    "finally:\n",
    "    # Always close open file handles to avoid resource leaks.\n",
    "    for _, fh in files:\n",
    "        fh.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cddab84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index_name': 'fruit_index_combined_final'}\n"
     ]
    }
   ],
   "source": [
    "print({\"index_name\": INDEX_NAME})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b3c206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPAIR: 200\n",
      "{\"status\":\"success\",\"results\":[{\"value\":\"red\",\"table_name\":\"table_1.csv\",\"row_number\":\"0\",\"tuple\":{\"id\":1,\"fruit\":\"apple\",\"color\":\"red\"}},{\"value\":\"yellow\",\"table_name\":\"table_1.csv\",\"row_number\":\"1\",\"tuple\":{\"id\":2,\"fruit\":\"banana\",\"color\":\"yellow\"}},{\"value\":null,\"table_name\":null,\"row_number\":null,\"tuple\":null},{\"value\":\"green\",\"table_name\":null,\"row_number\":null,\"tuple\":null}]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 0: Backend setup\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Base URL for your FastAPI backend. \n",
    "# This backend provides the `/repair/` endpoint for performing automatic \n",
    "# data imputation (repairing missing or incorrect attribute values).\n",
    "BACKEND_URL = \"http://localhost:8000\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 1: Load and preprocess the test (incomplete) dataset\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Load the CSV containing incomplete or inconsistent data.\n",
    "# Example: \"test_repair.csv\" has a 'fruit' column and a 'color' column,\n",
    "# but some color values are missing or incorrect.\n",
    "df = pd.read_csv(\"test_repair.csv\")\n",
    "\n",
    "# Replace all NaN (missing) values with Python None for JSON compatibility.\n",
    "# This ensures FastAPI receives proper `null` values in the request.\n",
    "# Convert the DataFrame into a list of dictionaries — one dict per row.\n",
    "records = df.where(pd.notnull(df), None).to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 2: Construct the repair request payload\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# The backend's `/repair/` endpoint expects a structured JSON object\n",
    "# describing what needs to be repaired and how to reason about it.\n",
    "payload = {\n",
    "    # High-level description of what this dataset represents.\n",
    "    # This helps the model (especially LLMs) understand the semantic context.\n",
    "    \"entity_description\": \"A small table of fruits and their colors\",\n",
    "\n",
    "    # Name of the column that contains missing or incorrect values.\n",
    "    \"target_name\": \"color\",\n",
    "\n",
    "    # The list of target entries (the column to repair), where:\n",
    "    #   - 'id' is a unique identifier for each row.\n",
    "    #   - 'value' is the current value (may be None or incorrect).\n",
    "    \"target_data\": [\n",
    "        {\"id\": i, \"value\": r[\"color\"]} for i, r in enumerate(records)\n",
    "    ],\n",
    "\n",
    "    # The contextual columns (attributes that can help infer missing values).\n",
    "    # Here, 'fruit' acts as the pivot context used for reasoning.\n",
    "    \"pivot_names\": [\"fruit\"],\n",
    "\n",
    "    # The actual contextual values, structured the same way as 'target_data'.\n",
    "    # Each 'values' list contains the pivot column's value(s) for that row.\n",
    "    \"pivot_data\": [\n",
    "        {\"id\": i, \"values\": [r[\"fruit\"]]} for i, r in enumerate(records)\n",
    "    ],\n",
    "\n",
    "    # Name of the model or reasoning engine to use for inference.\n",
    "    # This must match one of the initialized models from your backend: must be from ['GPT-4-AzureOpenAI', 'GPT-4-OpenAI', 'Llama 3.1']\n",
    "    \"reasoner_name\": \"GPT-4-AzureOpenAI\",\n",
    "\n",
    "    # Name of the index to use for context retrieval.\n",
    "    # This should correspond to the one previously created with /index/ (e.g., \"fruit_index_combined_2\").\n",
    "    \"index_name\": INDEX_NAME,\n",
    "\n",
    "    # Which retrieval engine(s) to use:\n",
    "    #   - \"semantic\" uses Qdrant (vector search)\n",
    "    #   - \"syntactic\" uses Elasticsearch (keyword/fuzzy search)\n",
    "    #   - \"both\" uses a combination of both\n",
    "    \"index_type\": \"semantic\",\n",
    "\n",
    "    # Optional reranker model (None disables reranking).\n",
    "    # Could later support models like \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    \"reranker_type\": None\n",
    "}\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 3: Send the repair request to the backend\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# POST the payload to the backend’s `/repair/` endpoint.\n",
    "# The backend will:\n",
    "#   1. Retrieve relevant context rows from Qdrant/Elasticsearch.\n",
    "#   2. Pass the context and missing entries to the reasoning model.\n",
    "#   3. Return imputed (repaired) values for the missing attributes.\n",
    "resp = requests.post(f\"{BACKEND_URL}/repair/\", json=payload)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 4: Display the results\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Print HTTP status code to confirm whether the repair succeeded.\n",
    "# Expected: 200 OK with a JSON payload containing imputed values.\n",
    "print(\"REPAIR:\", resp.status_code)\n",
    "\n",
    "# Print the full JSON response for inspection.\n",
    "# Example output:\n",
    "# {\n",
    "#   \"status\": \"success\",\n",
    "#   \"results\": [\n",
    "#       {\"value\": \"red\", \"table_name\": null, \"row_number\": null, \"tuple\": null},\n",
    "#       {\"value\": \"yellow\", ...}, ...\n",
    "#   ]\n",
    "# }\n",
    "print(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c4b304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'value': 'red',\n",
       "  'table_name': 'table_1.csv',\n",
       "  'row_number': '0',\n",
       "  'tuple': {'id': 1, 'fruit': 'apple', 'color': 'red'}},\n",
       " {'value': 'yellow',\n",
       "  'table_name': 'table_1.csv',\n",
       "  'row_number': '1',\n",
       "  'tuple': {'id': 2, 'fruit': 'banana', 'color': 'yellow'}},\n",
       " {'value': None, 'table_name': None, 'row_number': None, 'tuple': None},\n",
       " {'value': 'green', 'table_name': None, 'row_number': None, 'tuple': None}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### How to Parse the model response\n",
    "eval(resp.text.replace('null', 'None'))['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bda00e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fruit': 'apple', 'color': None},\n",
       " {'fruit': 'banana', 'color': 'blue'},\n",
       " {'fruit': 'avocado', 'color': None},\n",
       " {'fruit': 'lime', 'color': None}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
